{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions to students:\n",
    "\n",
    "1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.\n",
    "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
    "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
    "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
    "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
    "    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)\n",
    "    \n",
    "2. You are not allowed to insert new cells in the submitted notebook.\n",
    "\n",
    "3. You are not allowed to import any extra packages.\n",
    "\n",
    "4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.\n",
    "\n",
    "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. \n",
    "\n",
    "6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.\n",
    "\n",
    "7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with \"run all\" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.\n",
    "\n",
    "8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.\n",
    "\n",
    "9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :\"X_train, Y_train, X_test, Y_test\". In that order. The meaning of the 4 arrays can be easily inferred from their names.\n",
    "\n",
    "10. All plots must be labelled properly, all tables must have rows and columns named properly.\n",
    "\n",
    "11. Before subbmission ensure that you submit with the outputs (do not clear the outputs), so that when evaluating we can run selectively.\n",
    "\n",
    "12. Before submission ensure that the path for the folder containing the data is \"../../Data/\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression \n",
    "\n",
    "Write code for doing logistic regression below. Also write code for choosing best hyperparameters for each kernel type (use a part of training set as validation set). \n",
    "\n",
    "The range of hyperparameters is typically chosen on a log scale e.g. 1e-4, 1e-3, 1e-2... 1e3.\n",
    "\n",
    "Write code for running in the cell after (You may be asked to demonstrate your code during the viva using this cell.)\n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For each classification data set report the best kernel and regularisation parameters for linear, RBF and Poly kernels. (Linear has no kernel parameter.) Report the training and test zero-one error for those hyperparameters. \n",
    "\n",
    "For each given hyperparameter setting (kernel and regularisation) you will have to do some exploring to find the right learning rate to use in gradient descent. The optimisation learning rate is not a model hyperparameter and hence can be chosen based on just the training set. i.e. choose the learning rate for which the training loss decreases the most.\n",
    "\n",
    "For the synthetic classification datasets (dataset_A and dataset_B) in 2-dimensions, also illustrate the learned classifier for each kernel setting. Do this in the last codeWrite cell for this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite \n",
    "# Write logistic regression code from scratch. Use gradient descent.\n",
    "# Only write functions here\n",
    "\n",
    "def calculate_kernel_matrix(X, kernel, kernel_param):\n",
    "    \"\"\"\n",
    "    Calculate the kernel matrix for the data X\n",
    "    \"\"\"\n",
    "    if(kernel == 'linear'):\n",
    "        return np.matmul(X, X.T)\n",
    "    elif(kernel == 'poly'):\n",
    "        return (1 + np.matmul(X, X.T)) ** kernel_param\n",
    "    elif(kernel == 'rbf'):\n",
    "        # kernel(X, Y) = exp(-gamma * ||X - Y||^2)\n",
    "        return (np.exp(-kernel_param * (np.sum(X**2, axis=1)[:, np.newaxis] + np.sum(X**2, axis=1) - 2 * np.matmul(X, X.T))))\n",
    "\n",
    "def calculate_kernel_values(X, x, kernel, kernel_param):\n",
    "    \"\"\"\n",
    "    Calculate the kernel values between x and all the points in X\n",
    "    \"\"\"\n",
    "    if kernel == 'linear':\n",
    "        return np.dot(X, x)\n",
    "    elif kernel == 'poly':\n",
    "        return (1 + np.dot(X, x)) ** kernel_param\n",
    "    elif kernel == 'rbf':\n",
    "        return np.exp(-kernel_param * np.sum((X - x) ** 2, axis=1))\n",
    "\n",
    "def train_pred_logistic_regression(X, Y, kernel='linear', reg_param=0., kernel_param=1., num_iter_gd=100):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X : (n,d) shape numpy array\n",
    "    Y : (n,)  shape numpy array\n",
    "    X_test : (m,d) shape numpy array\n",
    "    kernel = 'linear' or 'rbf' or 'poly' \n",
    "    reg_param = $\\ lambda$\n",
    "    num_iter_gd = number of GD iterations.\n",
    "\n",
    "    Returns the result of kernel logistic regression :\n",
    "    alpha: Vector of solutions for the dual. Numpy array of shape (n,)\n",
    "\n",
    "    Primal problem:\n",
    "    $ \\ min_w  \\ sum_{i=1}^n \\ log(1+\\ exp(-y_i* \\ w^\\ top \\phi(\\ x_i)))  + \\ frac{\\ lambda}{2} ||\\ w||^2 $\n",
    "\n",
    "    the dual of which is\n",
    "\n",
    "    $ \\ min_alpha \\ sum_{i=1}^n \\ log(1+\\ exp(-y_i* \\ alpha^\\ top K_{:,i} ))  + \\ frac{\\ lambda}{2} \\ alpha^\\ top K \\ alpha $\n",
    "    where $\\ phi$ is the feature got by the kernel.\n",
    "\n",
    "    Where K is the nxn kernel matrix computed on the training data.\n",
    "\n",
    "    The kernel is defined by the kernel_param:\n",
    "    If kernel=linear: K(\\ u,\\ v) = \\ u^\\ top \\ v  \n",
    "    If kernel=poly:  K(\\ u,\\ v) = (1+\\ u^\\ top \\ v)^(kernel_param)\n",
    "    If kernel=rbf:  K(\\ u,\\ v) = \\ exp(-kernel_param*||\\ u-\\ v||^2)\n",
    "    \"\"\"\n",
    "    kernel_X = calculate_kernel_matrix(X, kernel, kernel_param)\n",
    "    alpha = np.zeros(X.shape[0])\n",
    "    eta = 0.1\n",
    "    \n",
    "    for i in range(num_iter_gd):\n",
    "        delta_R_alpha = reg_param * alpha - np.sum((1 - 1 / (1 + np.exp(-Y * np.matmul(kernel_X, alpha)))) * Y * kernel_X, axis=1)\n",
    "        alpha = alpha - eta * delta_R_alpha\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def test_pred(alpha, train_X, train_Y, test_X, kernel, kernel_param):\n",
    "    \"\"\"\n",
    "    Return the predictions on test_X using the learnt alphas\n",
    "    \"\"\"\n",
    "    return np.array([np.sign(np.dot(alpha, calculate_kernel_values(train_X, test_X[i], kernel, kernel_param))) for i in range(test_X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A\n",
      "\n",
      "Kernel: linear, best_kernel_param: 1, best_reg_param: 1e-05, best_val_accuracy: 0.8266666666666667\n",
      "Test accuracy: 0.806\n",
      "\n",
      "Kernel: poly, best_kernel_param: 2, best_reg_param: 0.0001, best_val_accuracy: 0.87\n",
      "Test accuracy: 0.86\n",
      "\n",
      "Kernel: rbf, best_kernel_param: 1, best_reg_param: 1e-06, best_val_accuracy: 1.0\n",
      "Test accuracy: 0.994\n",
      "\n",
      "Dataset B\n",
      "\n",
      "Kernel: linear, best_kernel_param: 1, best_reg_param: 1e-06, best_val_accuracy: 0.74\n",
      "Test accuracy: 0.758\n",
      "\n",
      "Kernel: poly, best_kernel_param: 4, best_reg_param: 1e-05, best_val_accuracy: 0.8333333333333334\n",
      "Test accuracy: 0.776\n",
      "\n",
      "Kernel: rbf, best_kernel_param: 2, best_reg_param: 0.0001, best_val_accuracy: 0.8333333333333334\n",
      "Test accuracy: 0.784\n",
      "\n",
      "Dataset C\n",
      "\n",
      "Kernel: linear, best_kernel_param: 1, best_reg_param: 1e-06, best_val_accuracy: 0.5333333333333333\n",
      "Test accuracy: 0.5016835016835017\n",
      "\n",
      "Kernel: poly, best_kernel_param: 8, best_reg_param: 0.01, best_val_accuracy: 0.96\n",
      "Test accuracy: 0.9494949494949495\n",
      "\n",
      "Kernel: rbf, best_kernel_param: 0.1, best_reg_param: 1e-06, best_val_accuracy: 0.9933333333333333\n",
      "Test accuracy: 0.9663299663299664\n",
      "\n",
      "Dataset D\n",
      "\n",
      "Kernel: linear, best_kernel_param: 1, best_reg_param: 10, best_val_accuracy: 0.7\n",
      "Test accuracy: 0.7692307692307693\n",
      "\n",
      "Kernel: poly, best_kernel_param: 6, best_reg_param: 0.1, best_val_accuracy: 0.7625\n",
      "Test accuracy: 0.7988165680473372\n",
      "\n",
      "Kernel: rbf, best_kernel_param: 0.1, best_reg_param: 1e-06, best_val_accuracy: 0.8375\n",
      "Test accuracy: 0.8520710059171598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CodeWrite : Use the functions above to do validation to get best hyperparameters \n",
    "# (i.e. kernel_param and regularisation_param).\n",
    "# Also, get the numbers you report below.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "num_iter = 100\n",
    "\n",
    "best_alphas_A = {}\n",
    "best_alphas_B = {}\n",
    "best_kernel_param_A = {}\n",
    "best_kernel_param_B = {}\n",
    "best_reg_param_A = {}\n",
    "best_reg_param_B = {}\n",
    "\n",
    "def find_best_parameter(kernel, X_train, Y_train, X_val, Y_val):\n",
    "    best_kernel_param, best_reg_param, best_accuracy = 0, 0, 0\n",
    "\n",
    "    if kernel == 'linear':\n",
    "        kernel_params = [1]\n",
    "        reg_params = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "    elif kernel == 'poly':\n",
    "        kernel_params = [1, 2, 4, 6, 8, 10]\n",
    "        reg_params = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "    elif kernel == 'rbf':\n",
    "        kernel_params = [1e-1, 1, 2, 4, 10, 100]\n",
    "        reg_params = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "\n",
    "    for kernel_param in kernel_params:\n",
    "        for reg_param in reg_params:\n",
    "            alpha = train_pred_logistic_regression(X_train, Y_train, kernel=kernel, reg_param=reg_param, kernel_param=kernel_param, num_iter_gd=num_iter)\n",
    "            pred = test_pred(alpha, X_train, Y_train, X_val, kernel=kernel, kernel_param=kernel_param)\n",
    "            accuracy = np.mean(pred == Y_val)\n",
    "            # print(\"Kernel: {}, kernel_param: {}, reg_param: {}, val_accuracy: {}\".format(kernel, kernel_param, reg_param, accuracy))\n",
    "            if accuracy > best_accuracy:\n",
    "                best_kernel_param, best_reg_param, best_accuracy = kernel_param, reg_param, accuracy\n",
    "    return best_kernel_param, best_reg_param, best_accuracy\n",
    "\n",
    "for a in ['A', 'B', 'C', 'D']:\n",
    "    data = np.load('../../Data/dataset_{}.npz'.format(a))\n",
    "    X_train, Y_train = data['arr_0'], data['arr_1']\n",
    "    X_test, Y_test = data['arr_2'], data['arr_3']\n",
    "\n",
    "    print(\"Dataset {}\\n\".format(a))\n",
    "    X_train, X_val = X_train[:int(0.8 * X_train.shape[0])], X_train[int(0.8 * X_train.shape[0]):]\n",
    "    Y_train, Y_val = Y_train[:int(0.8 * Y_train.shape[0])], Y_train[int(0.8 * Y_train.shape[0]):]\n",
    "\n",
    "    for kernel in kernels:\n",
    "        best_kernel_param, best_reg_param, best_accuracy = find_best_parameter(kernel, X_train, Y_train, X_val, Y_val)\n",
    "        print(\"Kernel: {}, best_kernel_param: {}, best_reg_param: {}, best_val_accuracy: {}\".format(kernel, best_kernel_param, best_reg_param, best_accuracy))\n",
    "\n",
    "        alpha = train_pred_logistic_regression(X_train, Y_train, kernel, best_reg_param, best_kernel_param)\n",
    "\n",
    "        if(a == 'A'):\n",
    "            best_alphas_A[kernel], best_kernel_param_A[kernel], best_reg_param_A[kernel] = alpha, best_kernel_param, best_reg_param\n",
    "        elif(a == 'B'):\n",
    "            best_alphas_B[kernel], best_kernel_param_B[kernel], best_reg_param_B[kernel] = alpha, best_kernel_param, best_reg_param\n",
    "\n",
    "        pred = test_pred(alpha, X_train, Y_train, X_test, kernel, best_kernel_param)\n",
    "        accuracy = np.mean(pred == Y_test)\n",
    "        print(\"Test accuracy: {}\\n\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite Cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error for all three kernel choices, for all 4 datasets (2 real world and 2 synthetic).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell: Generate plots of learned classifier for all three kernel types, on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 3 * 2 = 6\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "def plot(alpha, X_test, Y_test, kernel, kernel_param, reg_param, dataset):\n",
    "    minX, maxX = np.min(X_test[:, 0]), np.max(X_test[:, 0])\n",
    "    minY, maxY = np.min(X_test[:, 1]), np.max(X_test[:, 1])\n",
    "    points = 500\n",
    "    X, Y = np.meshgrid(np.linspace(minX - 0.2, maxX + 0.2, points), np.linspace(minY - 0.2, maxY + 0.2, points))\n",
    "    Z = np.zeros((points, points))\n",
    "    X_plotter = np.zeros((points * points, 2))\n",
    "    for i in range(points):\n",
    "        for j in range(points):\n",
    "            X_plotter[i * points + j, 0] = X[i, j]\n",
    "            X_plotter[i * points + j, 1] = Y[i, j]\n",
    "    pred = test_pred(alpha, X_train, Y_train, X_plotter, kernel=kernel, kernel_param=kernel_param)\n",
    "    Z = pred.reshape((points, points))\n",
    "    plt.contourf(X, Y, Z, alpha=0.2, colors=['r', 'g'])\n",
    "    plt.title(\"{} - Kernel: {}, KP: {}, RP: {}\".format(dataset, kernel, kernel_param, reg_param))\n",
    "    plt.scatter(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], marker='.', color='g')\n",
    "    plt.scatter(X_test[Y_test == -1, 0], X_test[Y_test == -1, 1], marker='.', color='r')\n",
    "    plt.show()\n",
    "\n",
    "for a in ['A', 'B']:\n",
    "    data = np.load('../../Data/dataset_{}.npz'.format(a))\n",
    "    X_train = data['arr_0']\n",
    "    Y_train = data['arr_1']\n",
    "    X_test = data['arr_2']\n",
    "    Y_test = data['arr_3']\n",
    "\n",
    "    X_train = X_train[:int(X_train.shape[0] * 0.8)]\n",
    "    Y_train = Y_train[:int(Y_train.shape[0] * 0.8)]\n",
    "    \n",
    "    print(\"Dataset {}\".format(a))\n",
    "    for kernel in kernels:\n",
    "        if(a == 'A'):\n",
    "            alpha = best_alphas_A[kernel]\n",
    "            kernel_param = best_kernel_param_A[kernel]\n",
    "            reg_param = best_reg_param_A[kernel]\n",
    "        else:\n",
    "            alpha = best_alphas_B[kernel]\n",
    "            kernel_param = best_kernel_param_B[kernel]\n",
    "            reg_param = best_reg_param_B[kernel]\n",
    "        plot(alpha, X_test, Y_test, kernel, kernel_param, reg_param, \"Dataset {}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM\n",
    "\n",
    "Write code for learning SVM below. Also write code for choosing best hyperparameters for each kernel type. You may use sklearn.svm for this purpose. (use a part of training set as validation set)\n",
    "\n",
    "Write code for running in the cell after (You may be asked to demonstrate your code during the viva using this cell.)\n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For each classification data set report the best kernel and regularisation parameters for linear, RBF and Poly kernels. (Linear has no kernel parameter.) Report the training and test zero-one error for those hyperparameters.\n",
    "\n",
    "For the synthetic classification datasets in 2-dimensions, also illustrate the learned classifier for each kernel setting. Do this in the last codeWrite cell for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write SVM classifier using SKlearn\n",
    "# write only functions here\n",
    "\n",
    "def train_svm(X, Y, kernel='rbf', reg_param=1.0, kernel_param=1.0):\n",
    "    \"\"\"\n",
    "    Train SVM classifier using sklearn.svm.SVC\n",
    "    \"\"\"\n",
    "    clf = svm.SVC(kernel=kernel, C=reg_param)\n",
    "    if kernel == 'rbf':\n",
    "        clf.set_params(**{'gamma': kernel_param})\n",
    "    elif kernel == 'poly':\n",
    "        clf.set_params(**{'degree': kernel_param, 'coef0': 1.0})\n",
    "    clf.fit(X, Y)\n",
    "    return clf\n",
    "\n",
    "def predict_value(clf, X):\n",
    "    \"\"\"\n",
    "    Predict using the SVM classifier\n",
    "    \"\"\"\n",
    "    return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write code here for doing validation (for kernel_param and regularisation_param)\n",
    "# on a subset of the training set. \n",
    "# Also for generating the numbers that you report below.\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "best_clf_A = {}\n",
    "best_clf_B = {}\n",
    "best_kernel_param_A = {}\n",
    "best_reg_param_A = {}\n",
    "best_kernel_param_B = {}\n",
    "best_reg_param_B = {}\n",
    "\n",
    "def find_best_parameter(kernel, X_train, Y_train, X_val, Y_val):\n",
    "    best_kernel_param, best_reg_param, best_accuracy = None, None, 0\n",
    "    \n",
    "    if kernel == 'linear':\n",
    "        kernel_params = [1]\n",
    "        reg_params = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "    elif kernel == 'poly':\n",
    "        kernel_params = [1, 2, 4, 6, 8]\n",
    "        reg_params = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "    elif kernel == 'rbf':\n",
    "        kernel_params = [1e-3, 1e-2, 1e-1, 1, 2, 4, 10, 100]\n",
    "        reg_params = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "\n",
    "    for kernel_param in kernel_params:\n",
    "        for reg_param in reg_params:\n",
    "            clf = train_svm(X_train, Y_train, kernel, reg_param=reg_param, kernel_param=kernel_param)\n",
    "            pred = predict_value(clf, X_val)\n",
    "            accuracy = np.mean(pred == Y_val)\n",
    "            print(\"kernel: {}, kernel_param: {}, reg_param: {}, accuracy: {}\".format(kernel, kernel_param, reg_param, accuracy))\n",
    "            if accuracy > best_accuracy:\n",
    "                best_kernel_param, best_reg_param, best_accuracy = kernel_param, reg_param, accuracy\n",
    "    return best_kernel_param, best_reg_param, best_accuracy\n",
    "\n",
    "for a in ['A', 'B', 'C', 'D']:\n",
    "    dataset = \"../../Data/dataset_\" + a + \".npz\"\n",
    "    data = np.load(dataset)\n",
    "\n",
    "    print(\"Dataset {}\\n\".format(a))\n",
    "    X_train, Y_train = data['arr_0'], data['arr_1']\n",
    "    X_test, Y_test = data['arr_2'], data['arr_3']\n",
    "\n",
    "    rand_indices = np.random.permutation(X_train.shape[0])\n",
    "    X_train, X_val = X_train[rand_indices[:int(X_train.shape[0] * 0.8)]], X_train[rand_indices[int(X_train.shape[0] * 0.8):]]\n",
    "    Y_train, Y_val = Y_train[rand_indices[:int(Y_train.shape[0] * 0.8)]], Y_train[rand_indices[int(Y_train.shape[0] * 0.8):]]\n",
    "\n",
    "    for kernel in kernels:\n",
    "        best_kernel_param, best_reg_param, best_accuracy = find_best_parameter(kernel, X_train, Y_train, X_val, Y_val)\n",
    "        print(\"Kernel: {}, best_kernel_param: {}, best_reg_param: {}, val_accuracy: {}\".format(kernel, best_kernel_param, best_reg_param, best_accuracy))\n",
    "        clf = train_svm(X_train, Y_train, kernel=kernel, reg_param=best_reg_param, kernel_param=best_kernel_param)\n",
    "        if(a == 'A'):\n",
    "            best_clf_A[kernel], best_kernel_param_A[kernel], best_reg_param_A[kernel] = clf, best_kernel_param, best_reg_param\n",
    "        elif(a == 'B'):\n",
    "            best_clf_B[kernel], best_kernel_param_B[kernel], best_reg_param_B[kernel] = clf, best_kernel_param, best_reg_param\n",
    "                          \n",
    "        pred = predict_value(clf, X_test)\n",
    "        accuracy = np.mean(pred == Y_test)\n",
    "        print(\"Test accuracy: {}\\n\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite Cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error for all three kernel choices, for all 4 datasets (2 real world and 2 synthetic).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell: Generate plots of learned classifier for all three kernel types, on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 3 * 2 = 6\n",
    "\n",
    "def plot(clf, X_test, Y_test, dataset, kernel, kernel_param, reg_param):\n",
    "    minX, maxX = np.min(X_test[:, 0]), np.max(X_test[:, 0])\n",
    "    minY, maxY = np.min(X_test[:, 1]), np.max(X_test[:, 1])\n",
    "    points = 500\n",
    "    X, Y = np.meshgrid(np.linspace(minX - 0.2, maxX + 0.2, points), np.linspace(minY - 0.2, maxY + 0.2, points))\n",
    "    Z = np.zeros((points, points))\n",
    "    for i in range(points):\n",
    "        for j in range(points):\n",
    "            Z[i][j] = clf.predict(np.array([[X[i][j], Y[i][j]]]))\n",
    "    plt.contourf(X, Y, Z, alpha=0.2, colors=['r', 'g'])\n",
    "    plt.title(\"{}, Kernel: {}, KP: {}, RP: {}\".format(dataset, kernel, kernel_param, reg_param))\n",
    "    plt.scatter(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], marker='.', color='g')\n",
    "    plt.scatter(X_test[Y_test == -1, 0], X_test[Y_test == -1, 1], marker='.', color='r')\n",
    "    plt.show()\n",
    "\n",
    "for a in ['A', 'B']:\n",
    "    data = np.load('../../Data/dataset_{}.npz'.format(a))\n",
    "    X_train = data['arr_0']\n",
    "    Y_train = data['arr_1']\n",
    "    X_test = data['arr_2']\n",
    "    Y_test = data['arr_3']\n",
    "\n",
    "    X_train = X_train[:int(X_train.shape[0] * 0.8)]\n",
    "    Y_train = Y_train[:int(Y_train.shape[0] * 0.8)]\n",
    "    \n",
    "    print(\"Dataset {}\".format(a))\n",
    "    for kernel in kernels:\n",
    "        if(a == 'A'):\n",
    "            clf, kernel_param, reg_param = best_clf_A[kernel], best_kernel_param_A[kernel], best_reg_param_A[kernel]\n",
    "        elif(a == 'B'):\n",
    "            clf, kernel_param, reg_param = best_clf_B[kernel], best_kernel_param_B[kernel], best_reg_param_B[kernel]\n",
    "        plot(clf, X_test, Y_test, \"Dataset {}\".format(a), kernel, kernel_param, reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Decision Tree\n",
    "\n",
    "Write code for learning decision tree below. Take as an argument a hyperparameter on what size node to stop splitting. Use a part of training set as validation set.\n",
    "\n",
    "Write code for running in the cell after (You may be asked to demonstrate your code during the viva using this cell.)\n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all four data sets  report the best node size to stop splitting. Report the training and test zero-one error for those hyperparameters.\n",
    "\n",
    "For datasets A and B, also illustrate the learned classifier. Do this in the last codeWrite cell for this question.\n",
    "\n",
    "Important: Think about how you will represent a decision tree. (Possible soln: Store as a list of tuples containing node position, attribute to split, threshold, class to classifiy (if leaf node) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write Decision tree classifier from scratch, \n",
    "# write only functions here (you may write extra functions here if you wish)\n",
    "\n",
    "def find_entropy(X_left_count, X_right_count, X_left_pos, X_right_pos):\n",
    "    if(X_left_count == 0 or X_right_count == 0):\n",
    "        return 0\n",
    "    if(X_left_pos == 0 or X_right_pos == 0):\n",
    "        return 0\n",
    "    X_left_neg = X_left_count - X_left_pos\n",
    "    X_right_neg = X_right_count - X_right_pos\n",
    "    entropy_left, entropy_right = 0, 0\n",
    "    entropy_left += -1 * (X_left_pos / X_left_count) * np.log2(X_left_pos / X_left_count)\n",
    "    entropy_left += -1 * (X_left_neg / X_left_count) * np.log2(X_left_neg / X_left_count)\n",
    "    entropy_right += -1 * (X_right_pos / X_right_count) * np.log2(X_right_pos / X_right_count)\n",
    "    entropy_right += -1 * (X_right_neg / X_right_count) * np.log2(X_right_neg / X_right_count)\n",
    "    return entropy_left * (X_left_count / (X_left_count + X_right_count)) + entropy_right * (X_right_count / (X_left_count + X_right_count))\n",
    "\n",
    "def find_accuracy(X_left_count, X_right_count, X_left_pos, X_right_pos):\n",
    "    X_left_neg = (X_left_count - X_left_pos)\n",
    "    X_right_neg = (X_right_count - X_right_pos)\n",
    "    total = X_left_count + X_right_count\n",
    "    return max((X_left_pos + X_right_neg) / total, (X_left_neg + X_right_pos) / total, (X_left_pos + X_right_pos) / total, (X_left_neg + X_right_neg) / total)\n",
    "\n",
    "def find_best_attribute_threshold(X, Y, criterion):\n",
    "    best_attribute, best_threshold = -1, -1\n",
    "    best_accuracy, best_entropy = 0, np.inf\n",
    "\n",
    "    for attribute in range(X.shape[1]):\n",
    "        sorted_thresholds = np.sort(np.unique(X[:, attribute]))\n",
    "        sorted_X, sorted_Y = np.sort(X[:, attribute]), Y[np.argsort(X[:, attribute])]\n",
    "        X_left_count, X_right_count = 0, X.shape[0]\n",
    "        X_left_pos, X_right_pos = 0, np.sum(sorted_Y == 1)\n",
    "        for threshold in sorted_thresholds:\n",
    "            while(X_left_count < X.shape[0] and sorted_X[X_left_count] < threshold):\n",
    "                X_left_count += 1\n",
    "                X_right_count -= 1\n",
    "                if(X_left_count < X.shape[0] and sorted_Y[X_left_count-1] == 1):\n",
    "                    X_left_pos += 1\n",
    "                    X_right_pos -= 1\n",
    "            if(criterion == 'entropy'):\n",
    "                entropy = find_entropy(X_left_count, X_right_count, X_left_pos, X_right_pos)\n",
    "                if(entropy < best_entropy):\n",
    "                    best_entropy, best_attribute, best_threshold = entropy, attribute, threshold\n",
    "            elif(criterion == 'accuracy'):\n",
    "                accuracy = find_accuracy(X_left_count, X_right_count, X_left_pos, X_right_pos)\n",
    "                if(accuracy > best_accuracy):\n",
    "                    best_accuracy, best_attribute, best_threshold = accuracy, attribute, threshold\n",
    "    return best_attribute, best_threshold\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute, type, value, threshold):\n",
    "        self.attribute = attribute\n",
    "        self.type = type # 0: leaf, 1: internal\n",
    "        self.value = value # if leaf, value is the class label, else None\n",
    "        self.threshold = threshold # if internal, threshold is the threshold, else None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def split_node(self, X, Y, criterion, num_nodes_stop):\n",
    "        if(X.shape[0] <= num_nodes_stop):\n",
    "            self.type = 0\n",
    "            self.value = np.sign(np.sum(Y))\n",
    "            self.value = 1 if self.value == 0 else self.value\n",
    "            return self\n",
    "\n",
    "        self.type = 1\n",
    "        self.attribute, self.threshold = find_best_attribute_threshold(X, Y, criterion)\n",
    "        X_left, X_right = X[X[:, self.attribute] < self.threshold], X[X[:, self.attribute] >= self.threshold]\n",
    "        Y_left, Y_right = Y[X[:, self.attribute] < self.threshold], Y[X[:, self.attribute] >= self.threshold]\n",
    "\n",
    "        if(X_left.shape == X.shape or X_right.shape == X.shape):\n",
    "            self.type = 0\n",
    "            self.value = np.sign(np.sum(Y))\n",
    "            self.value = 1 if self.value == 0 else self.value\n",
    "            return self\n",
    "\n",
    "        node_left, node_right = Node(None, None, None, None), Node(None, None, None, None)\n",
    "        self.left = node_left.split_node(X_left, Y_left, criterion, num_nodes_stop)\n",
    "        self.right = node_right.split_node(X_right, Y_right, criterion, num_nodes_stop)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if(self.type == 0):\n",
    "            return self.value\n",
    "        else:\n",
    "            if(X[self.attribute] < self.threshold):\n",
    "                return self.left.predict(X)\n",
    "            else:\n",
    "                return self.right.predict(X)\n",
    "\n",
    "def train_decision_tree(X, Y, num_nodes_stop=1, criterion='accuracy'):\n",
    "    \"\"\" \n",
    "    Returns a decision tree trained on X and Y. \n",
    "    Stops splitting nodes when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Returns a tree (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    root = Node(None, 0, None, None)\n",
    "    root.split_node(X, Y, criterion, num_nodes_stop)\n",
    "    return root  \n",
    "\n",
    "def eval_decision_tree(tree, test_X):\n",
    "    \"\"\" \n",
    "    Takes in a tree, and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\n",
    "    \"\"\"\n",
    "    return np.array([tree.predict(x) for x in test_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 1\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 1\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 2\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 2\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 3\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 3\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 4\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 4\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 5\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 5\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 6\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 6\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 7\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 7\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 8\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 8\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 9\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 9\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bersi\\AppData\\Local\\Temp\\ipykernel_8600\\2830503370.py:16: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy_right += -1 * (X_right_neg / X_right_count) * np.log2(X_right_neg / X_right_count)\n",
      "C:\\Users\\bersi\\AppData\\Local\\Temp\\ipykernel_8600\\2830503370.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  entropy_right += -1 * (X_right_neg / X_right_count) * np.log2(X_right_neg / X_right_count)\n",
      "C:\\Users\\bersi\\AppData\\Local\\Temp\\ipykernel_8600\\2830503370.py:14: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy_left += -1 * (X_left_neg / X_left_count) * np.log2(X_left_neg / X_left_count)\n",
      "C:\\Users\\bersi\\AppData\\Local\\Temp\\ipykernel_8600\\2830503370.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  entropy_left += -1 * (X_left_neg / X_left_count) * np.log2(X_left_neg / X_left_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 10\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 11\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 11\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 12\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 12\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 13\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 13\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 14\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 14\n",
      "Accuracy: 0.898, Criterion: accuracy, Num_nodes_stop: 15\n",
      "Accuracy: 0.486, Criterion: entropy, Num_nodes_stop: 15\n",
      "Dataset B\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 1\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 1\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 2\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 2\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 3\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 3\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 4\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 4\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 5\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 5\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 6\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 6\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 7\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 7\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 8\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 8\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 9\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 9\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 10\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 10\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 11\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 11\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 12\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 12\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 13\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 13\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 14\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 14\n",
      "Accuracy: 0.79, Criterion: accuracy, Num_nodes_stop: 15\n",
      "Accuracy: 0.51, Criterion: entropy, Num_nodes_stop: 15\n",
      "Dataset C\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 1\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 1\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 2\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 2\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 3\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 3\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 4\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 4\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 5\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 5\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 6\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 6\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 7\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 7\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 8\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 8\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 9\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 9\n",
      "Accuracy: 0.8484848484848485, Criterion: accuracy, Num_nodes_stop: 10\n",
      "Accuracy: 0.5117845117845118, Criterion: entropy, Num_nodes_stop: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m num_nodes_stop \u001b[39min\u001b[39;00m num_nodes_stops:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m criterion \u001b[39min\u001b[39;00m criterions:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         tree \u001b[39m=\u001b[39m train_decision_tree(X_train, Y_train, num_nodes_stop, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         Y_pred \u001b[39m=\u001b[39m eval_decision_tree(tree, X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(Y_pred \u001b[39m==\u001b[39m Y_test)\n",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain_decision_tree\u001b[1;34m(X, Y, num_nodes_stop, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mReturns a decision tree trained on X and Y. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mStops splitting nodes when a node has hit a size of \"num_nodes_stop\" or lower.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mSplit criterion can be either 'accuracy' or 'entropy'.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mReturns a tree (In whatever format that you find appropriate)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m root \u001b[39m=\u001b[39m Node(\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m root\u001b[39m.\u001b[39;49msplit_node(X, Y, criterion, num_nodes_stop)\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mreturn\u001b[39;00m root\n",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36mNode.split_node\u001b[1;34m(self, X, Y, criterion, num_nodes_stop)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m node_left, node_right \u001b[39m=\u001b[39m Node(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m), Node(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft \u001b[39m=\u001b[39m node_left\u001b[39m.\u001b[39;49msplit_node(X_left, Y_left, criterion, num_nodes_stop)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m node_right\u001b[39m.\u001b[39msplit_node(X_right, Y_right, criterion, num_nodes_stop)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36mNode.split_node\u001b[1;34m(self, X, Y, criterion, num_nodes_stop)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m node_left, node_right \u001b[39m=\u001b[39m Node(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m), Node(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft \u001b[39m=\u001b[39m node_left\u001b[39m.\u001b[39;49msplit_node(X_left, Y_left, criterion, num_nodes_stop)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m node_right\u001b[39m.\u001b[39msplit_node(X_right, Y_right, criterion, num_nodes_stop)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36mNode.split_node\u001b[1;34m(self, X, Y, criterion, num_nodes_stop)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m find_best_attribute_threshold(X, Y, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m X_left, X_right \u001b[39m=\u001b[39m X[X[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold], X[X[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m Y_left, Y_right \u001b[39m=\u001b[39m Y[X[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold], Y[X[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold]\n",
      "\u001b[1;32mf:\\5th Semester  - IITM\\CS5691\\CS5691-PRML\\Assignment-2\\Notebook\\ProgAss2_Student.ipynb Cell 15\u001b[0m in \u001b[0;36mfind_best_attribute_threshold\u001b[1;34m(X, Y, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m best_accuracy, best_entropy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39minf\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m attribute \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     sorted_thresholds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(np\u001b[39m.\u001b[39;49munique(X[:, attribute]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     sorted_X, sorted_Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(X[:, attribute]), Y[np\u001b[39m.\u001b[39margsort(X[:, attribute])]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/5th%20Semester%20%20-%20IITM/CS5691/CS5691-PRML/Assignment-2/Notebook/ProgAss2_Student.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     X_left_count, X_right_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bersi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bersi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[0;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CodeWrite cell\n",
    "# Write code here for doing validation to find the best hyperparameters (i.e. num_nodes_stop)\n",
    "# Also Generate the numbers that you report below. \n",
    "# Repeat with criterion set to entropy also.\n",
    "\n",
    "def plot(tree, X_test, Y_test, criterion, num_nodes_stop, dataset):\n",
    "    minX, maxX = np.min(X_test[:, 0]), np.max(X_test[:, 0])\n",
    "    minY, maxY = np.min(X_test[:, 1]), np.max(X_test[:, 1])\n",
    "\n",
    "    X, Y = np.meshgrid(np.linspace(minX - 0.2, maxX + 0.2, 300), np.linspace(minY - 0.2, maxY + 0.2, 300))\n",
    "    Z = np.array([tree.predict(np.array([x, y])) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
    "    Z = Z.reshape(X.shape)\n",
    "    plt.contourf(X, Y, Z, alpha=0.2, colors=['red', 'blue'])\n",
    "    plt.title('{} with criterion = {} and num_nodes_stop = {}'.format(dataset, criterion, num_nodes_stop))\n",
    "    plt.scatter(X_test[Y_test == 1, 0], X_test[Y_test == 1, 1], color='blue', marker='.', label='Class 1')\n",
    "    plt.scatter(X_test[Y_test == -1, 0], X_test[Y_test == -1, 1], color='red', marker='.', label='Class -1')\n",
    "    plt.show() \n",
    "\n",
    "for a in ['A', 'B', 'C', 'D']:\n",
    "    data = np.load(\"../../Data/dataset_{}.npz\".format(a))\n",
    "    X_train, Y_train = data['arr_0'], data['arr_1']\n",
    "    X_test, Y_test = data['arr_2'], data['arr_3']\n",
    "    print('Dataset {}'.format(a))\n",
    "\n",
    "    num_nodes_stops = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    criterions = ['accuracy', 'entropy']\n",
    "    for num_nodes_stop in num_nodes_stops:\n",
    "        for criterion in criterions:\n",
    "            tree = train_decision_tree(X_train, Y_train, num_nodes_stop, criterion)\n",
    "            Y_pred = eval_decision_tree(tree, X_test)\n",
    "            accuracy = np.mean(Y_pred == Y_test)\n",
    "            print(\"Accuracy: {}, Criterion: {}, Num_nodes_stop: {}\".format(accuracy, criterion, num_nodes_stop))\n",
    "            # if a == 'A' or a == 'B':\n",
    "            #     plot(tree, X_test, Y_test, criterion, num_nodes_stop, \"Dataset {}\".format(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error, for all 4 datasets (2 real world and 2 synthetic).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Codewrite cell: Generate plots of learned decision tree classifier on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Plots only required for the accuracy criterion.\n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 Random Forest classifier\n",
    "\n",
    "Write code for learning RandomForests below. Fix the following hyper parameters: (Fraction of data to learn tree=0.5, Fraction of number of features chosen in each node=0.5, num_nodes_stop=1).  Choose the number of trees to add in the forest by using a validation set. You may use a slightly modified version of the decision tree code you had written earlier.\n",
    "\n",
    "Write code for running in the cell after the nest. (You may be asked to demonstrate your code during the viva using this cell.) \n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all 4 data sets (A,B,C,D)  report the best number of trees found. Report the training and test zero-one error for those hyperparameters.\n",
    "\n",
    "For the synthetic classification datasets (datasets A and B) in 2-dimensions, also illustrate the learned classifier for each kernel setting. Do this in the last codeWrite cell for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write Random Forest classifier. \n",
    "\n",
    "def find_accuracy(X, Y, threshold, attribute):\n",
    "        X_left, X_right = X[X[:, attribute] < threshold], X[X[:, attribute] >= threshold]\n",
    "        Y_left, Y_right = Y[X[:, attribute] < threshold], Y[X[:, attribute] >= threshold]\n",
    "\n",
    "        pred_left, pred_right = np.sign(np.sum(Y_left)), np.sign(np.sum(Y_right))\n",
    "        pred_left = 1 if pred_left == 0 else pred_left\n",
    "        pred_right = 1 if pred_right == 0 else pred_right\n",
    "\n",
    "        accuracy_left, accuracy_right = np.mean(pred_left == Y_left), np.mean(pred_right == Y_right)\n",
    "        accuracy = (accuracy_left * X_left.shape[0] + accuracy_right * X_right.shape[0]) / X.shape[0]\n",
    "        return accuracy\n",
    "\n",
    "def find_entropy(X, Y, threshold, attribute):\n",
    "    X_left, X_right = X[X[:, attribute] < threshold], X[X[:, attribute] >= threshold]\n",
    "    Y_left, Y_right = Y[X[:, attribute] < threshold], Y[X[:, attribute] >= threshold]\n",
    "\n",
    "    entropy_left, entropy_right = 0, 0\n",
    "\n",
    "    for y in [-1, 1]:\n",
    "        prob = np.sum(Y_left == y) / Y_left.shape[0]\n",
    "        if(prob != 0):\n",
    "            entropy_left -= prob * np.log2(prob)\n",
    "        prob = np.sum(Y_right == y) / Y_right.shape[0]\n",
    "        if(prob != 0):\n",
    "            entropy_right -= prob * np.log2(prob)\n",
    "    \n",
    "    entropy = (entropy_left * X_left.shape[0] + entropy_right * X_right.shape[0]) / X.shape[0]\n",
    "    return entropy\n",
    "\n",
    "def find_best_attribute_threshold(X, Y, criterion):\n",
    "    best_attribute, best_threshold = -1, -1\n",
    "    best_accuracy, best_entropy = 0, np.inf\n",
    "\n",
    "    rand_perm = np.random.permutation(X.shape[1])\n",
    "    rand_perm = rand_perm[:int(0.5 * X.shape[1])]\n",
    "\n",
    "    for attribute in rand_perm:\n",
    "        for threshold in np.unique(X[:, attribute]):\n",
    "            if(criterion == 'entropy'):\n",
    "                entropy = find_entropy(X, Y, threshold, attribute)\n",
    "                if(entropy < best_entropy):\n",
    "                    best_entropy, best_attribute, best_threshold = entropy, attribute, threshold\n",
    "            elif(criterion == 'accuracy'):\n",
    "                accuracy = find_accuracy(X, Y, threshold, attribute)\n",
    "                if(accuracy > best_accuracy):\n",
    "                    best_accuracy, best_attribute, best_threshold = accuracy, attribute, threshold\n",
    "    return best_attribute, best_threshold\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute, type, value, threshold):\n",
    "        self.attribute = attribute\n",
    "        self.type = type # 0: leaf, 1: internal\n",
    "        self.value = value # if leaf, value is the class label, else None\n",
    "        self.threshold = threshold # if internal, threshold is the threshold, else None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def split_node(self, X, Y, criterion, num_nodes_stop):\n",
    "        if(X.shape[0] <= num_nodes_stop):\n",
    "            self.type = 0\n",
    "            self.value = np.sign(np.sum(Y))\n",
    "            self.value = 1 if self.value == 0 else self.value\n",
    "            return self\n",
    "\n",
    "        self.type = 1\n",
    "        self.attribute, self.threshold = find_best_attribute_threshold(X, Y, criterion)\n",
    "        X_left, X_right = X[X[:, self.attribute] < self.threshold], X[X[:, self.attribute] >= self.threshold]\n",
    "        Y_left, Y_right = Y[X[:, self.attribute] < self.threshold], Y[X[:, self.attribute] >= self.threshold]\n",
    "\n",
    "        node_left, node_right = Node(None, None, None, None), Node(None, None, None, None)\n",
    "        self.left = node_left.split_node(X_left, Y_left, criterion, num_nodes_stop)\n",
    "        self.right = node_right.split_node(X_right, Y_right, criterion, num_nodes_stop)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if(self.type == 0):\n",
    "            return self.value\n",
    "        else:\n",
    "            if(X[self.attribute] < self.threshold):\n",
    "                return self.left.predict(X)\n",
    "            else:\n",
    "                return self.right.predict(X)\n",
    "\n",
    "def train_decision_tree(X, Y, num_nodes_stop=1, criterion='accuracy'):\n",
    "    \"\"\" \n",
    "    Returns a decision tree trained on X and Y. \n",
    "    Stops splitting nodes when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Returns a tree (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    root = Node(None, 0, None, None)\n",
    "    root.split_node(X, Y, criterion, num_nodes_stop)\n",
    "    return root  \n",
    "\n",
    "def eval_decision_tree(tree, test_X):\n",
    "    \"\"\" \n",
    "    Takes in a tree, and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\n",
    "    \"\"\"\n",
    "    return np.array([tree.predict(x) for x in test_X])\n",
    "\n",
    "def train_random_forest(X, Y, num_trees=10, num_nodes_stop=1, criterion='accuracy', a=0.5, b=0.5):\n",
    "    \"\"\" \n",
    "    Returns a random forest trained on X and Y. \n",
    "    Trains num_trees.\n",
    "    Stops splitting nodes in each tree when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Fraction of data used per tree = a\n",
    "    Fraction of features used in each node = b\n",
    "    Returns a random forest (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def eval_random_forest(random_forest, test_X):\n",
    "    \"\"\" \n",
    "    Takes in a  random forest object (hhowever you want to store it), and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write code for choosing the best hyperparameters (num_trees, num_nodes_stop)\n",
    "# Write code here for generating the numbers that you report below.\n",
    "# Repeat above for criterion set to entropy also.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error, for all 4 datasets (2 real world and 2 synthetic).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Codewrite cell: Generate plots of learned Random Forest classifier on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Plots required only for the accuracy criterion.\n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 AdaBoost\n",
    "\n",
    "Write code for learning using AdaBoost below. Use 3 different weak learners below. (You may reuse code written above)\n",
    "\n",
    "1. 1 node decision tree \n",
    "2. Decision tree of fixed depth = 3 (Root, child, grand child)\n",
    "3. Decision tree of fixed depth = 7 (Root, child, grand child, ..., great^4 grand child)\n",
    "\n",
    "Run for 50 iterations. You may use the accuracy split criterion for all the three weak learners.\n",
    "\n",
    "Write code for running in the next cell. (You may be asked to demonstrate your code during the viva using this cell.) \n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all 4 data sets (A,B,C,D)  plot the train and test accuracy vs iterations. A total of 12 plots is expected. 4 datasets * 3 weak learners. Each plot should contain two curves, train and test error.  \n",
    "\n",
    "For the synthetic classification datasets (datasets A and B) in 2-dimensions, also illustrate the learned classifier for each weak learner setting. A total of 6 contourf style plots are expected here. Do this in the last codeWrite cell for this question.\n",
    "\n",
    "Summarise your observations in the last textwrite cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell\n",
    "# Write code to run here (no plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell \n",
    "# Plots for iteration vs error here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell \n",
    "# Plots for illustrating the classifier here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textwrite cell:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3a531bbf0f29f3151f5bd039b6fdd9153dda7bdef41c4c202a07430fcab450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
